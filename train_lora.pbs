#!/bin/bash
#PBS -N weatherlmm_train
#PBS -A UMCP0038
#PBS -j oe
#PBS -k eod
#PBS -q main
#PBS -l select=1:ncpus=64:ngpus=4:mem=128gb
#PBS -l walltime=08:00:00
#PBS -m abe
#PBS -M dcalhoun@umd.edu

# Load necessary modules
module purge
module load ncarenv-basic/25.10
module load conda
module load cuda/12.9.0
module load intel/2025.2.1

# Activate your Conda environment
conda activate weatherlmm

# Change to the directory from which the job was submitted
cd $PBS_O_WORKDIR

# Set environment variables
export NCCL_DEBUG=INFO
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Print job information
echo "=========================================="
echo "Job ID: $PBS_JOBID"
echo "Job Name: $PBS_JOBNAME"
echo "Node: $HOSTNAME"
echo "Working Directory: $PBS_O_WORKDIR"
echo "Start Time: $(date)"
echo "Number of GPUs: $(nvidia-smi -L | wc -l)"
echo "=========================================="

# Set paths
TRAIN_CSV="/glade/u/home/dcalhoun/cmsc723/manifests/manifest_train.csv"
EVAL_CSV="/glade/u/home/dcalhoun/cmsc723/manifests/manifest_test.csv"
OUTPUT_DIR="/glade/derecho/scratch/dcalhoun/checkpoints/weather_lora_$(date +%Y%m%d_%H%M%S)"

# Create output directory
mkdir -p "$OUTPUT_DIR"

# Run training
python -u LoRA_Training.py \
    --model_name Qwen/Qwen2.5-VL-3B-Instruct \
    --train_csv "$TRAIN_CSV" \
    --eval_csv "$EVAL_CSV" \
    --output_dir "$OUTPUT_DIR" \
    --image_size 384 \
    --num_train_epochs 4 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 1 \
    --learning_rate 1e-4 \
    --warmup_ratio 0.03 \
    --weight_decay 0.01 \
    --gradient_accumulation_steps 8 \ #modifying this from 16 to 8 due to memory constraints
    --gradient_checkpointing \
    --bf16 \
    --logging_steps 100 \
    --save_steps 400 \
    --save_total_limit 3 \
    --max_steps -1 \
    --use_8bit_optimizer

# Print completion information
echo "=========================================="
echo "End Time: $(date)"
echo "Training completed. Output saved to: $OUTPUT_DIR"
echo "=========================================="
