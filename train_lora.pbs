#!/bin/bash
#PBS -N weatherlmm_train
#PBS -A UMCP0038
#PBS -j oe
#PBS -k eod
#PBS -q main
#PBS -l select=1:ncpus=64:ngpus=4:mem=512GB
#PBS -l walltime=02:00:00
#PBS -m abe
#PBS -M dcalhoun@umd.edu

# Load necessary modules
module purge
module load ncarenv-basic/25.10
module load conda
module load cuda/12.9.0
module load intel/2025.2.1
module load openmpi/5.0.8

# Activate your Conda environment
conda activate weatherlmm

# Change to the directory from which the job was submitted
cd $PBS_O_WORKDIR

# Set environment variables for multi-GPU
# Don't restrict CUDA_VISIBLE_DEVICES - let all GPUs be visible
export NCCL_DEBUG=INFO
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Print job information
echo "=========================================="
echo "Job ID: $PBS_JOBID"
echo "Job Name: $PBS_JOBNAME"
echo "Node: $HOSTNAME"
echo "Working Directory: $PBS_O_WORKDIR"
echo "Start Time: $(date)"
echo "Number of GPUs: $(nvidia-smi -L | wc -l)"
echo "=========================================="

# Set paths
TRAIN_CSV="/glade/u/home/dcalhoun/cmsc723/manifests/manifest_train.csv"
EVAL_CSV="/glade/u/home/dcalhoun/cmsc723/manifests/manifest_test.csv"
OUTPUT_DIR="/glade/u/home/dcalhoun/cmsc723/checkpoints/weather_lora_multi_gpu_$(date +%Y%m%d_%H%M%S)"

# Create output directory
mkdir -p "$OUTPUT_DIR"

# Run training with multi-GPU support
# Trainer will automatically detect multiple GPUs and use DistributedDataParallel
# Model will be split across GPUs using device_map="auto"
python -u LoRA_Training.py \
    --model_name Qwen/Qwen2.5-VL-3B-Instruct \
    --train_csv "$TRAIN_CSV" \
    --eval_csv "$EVAL_CSV" \
    --output_dir "$OUTPUT_DIR" \
    --num_train_epochs 3 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --learning_rate 1e-4 \
    --warmup_ratio 0.03 \
    --weight_decay 0.01 \
    --gradient_accumulation_steps 4 \
    --fp16 \
    --logging_steps 10 \
    --save_steps 400 \
    --save_total_limit 3 \
    --max_steps -1 \
    --use_8bit_optimizer

# Print completion information
echo "=========================================="
echo "End Time: $(date)"
echo "Training completed. Output saved to: $OUTPUT_DIR"
echo "=========================================="

# Print instructions for viewing logs
echo ""
echo "To monitor training progress in real-time, run:"
echo "  tail -f weather_lmm_train_multi_gpu.o$PBS_JOBID"
echo ""
echo "TensorBoard logs are available at:"
echo "  $OUTPUT_DIR/logs"
echo ""
